{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 18:39:21.428097: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-29 18:39:21.508479: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-29 18:39:21.509296: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 18:39:22.687532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "IMG_H_SIZE = 32\n",
    "IMG_W_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para leer imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(dirname):\n",
    "    imgpath = dirname + os.sep\n",
    "    images = []\n",
    "    directories = []\n",
    "    dircount = []\n",
    "    prevRoot=''\n",
    "    cant=0\n",
    "    print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(imgpath):\n",
    "        for filename in filenames:\n",
    "            if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "                cant+=1\n",
    "                filepath = os.path.join(root, filename)\n",
    "                # image = plt.imread(filepath)\n",
    "                image = cv2.imread(filepath)\n",
    "                image = cv2.resize(image, (IMG_H_SIZE, IMG_W_SIZE))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                #Función de Normalización\n",
    "                # image = cv2.normalize(image, None, alpha=0,beta=200, norm_type=cv2.NORM_MINMAX)\n",
    "                images.append(image)\n",
    "                if prevRoot !=root:\n",
    "                    prevRoot=root\n",
    "                    directories.append(root)\n",
    "                    dircount.append(cant)\n",
    "                    cant=0\n",
    "    dircount.append(cant)\n",
    "\n",
    "    dircount = dircount[1:]\n",
    "    dircount[0]=dircount[0]+1\n",
    "    print('Directorios leidos:',len(directories))\n",
    "    print(\"Imagenes en cada directorio\", dircount)\n",
    "    print('Suma Total de imagenes en subdirs:',sum(dircount))\n",
    "\n",
    "    tipos=[]\n",
    "    indice=0\n",
    "    for directorio in directories:\n",
    "        name = directorio.split(os.sep)\n",
    "        print(indice , name[len(name)-1])\n",
    "        tipos.append(name[len(name)-1])\n",
    "        indice=indice+1\n",
    "\n",
    "    labels=[]\n",
    "    indice=0\n",
    "    for cantidad in dircount:\n",
    "        for i in range(cantidad):\n",
    "            labels.append(tipos[indice])\n",
    "        indice=indice+1\n",
    "\n",
    "    X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "    y = np.array(labels)\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos Sets de Entrenamiento y Test\n",
    "- En la función read_images, normalizamos cada imagen debido a que se encontró varias imágenes con un nivel de intensidad alto. Se utilizó esta ténica para obtener un mejor contraste en las imágenes con poco contraste debido al brillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo imagenes de  /home/jhonatan/Documentos/UIDE/TRATAMIENT/Clasificacion_Imagenes/CarneDataset/train/\n",
      "Directorios leidos: 7\n",
      "Imagenes en cada directorio [64, 213, 62, 204, 105, 37, 948]\n",
      "Suma Total de imagenes en subdirs: 1633\n",
      "0 CLASS_08\n",
      "1 CLASS_03\n",
      "2 CLASS_02\n",
      "3 CLASS_07\n",
      "4 CLASS_04\n",
      "5 CLASS_06\n",
      "6 CLASS_05\n",
      "leyendo imagenes de  /home/jhonatan/Documentos/UIDE/TRATAMIENT/Clasificacion_Imagenes/CarneDataset/test/\n",
      "Directorios leidos: 8\n",
      "Imagenes en cada directorio [28, 97, 48, 114, 45, 1, 19, 458]\n",
      "Suma Total de imagenes en subdirs: 810\n",
      "0 CLASS_08\n",
      "1 CLASS_03\n",
      "2 CLASS_02\n",
      "3 CLASS_07\n",
      "4 CLASS_04\n",
      "5 CLASS_01\n",
      "6 CLASS_06\n",
      "7 CLASS_05\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = read_images(os.path.join(os.getcwd(), 'CarneDataset/train'))\n",
    "X_test,y_test = read_images(os.path.join(os.getcwd(), 'CarneDataset/test'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepocesar las Imagenes - Normalizamos las imágenes de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificamos las etiquetas como enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el One-hot Encoding para la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_one_hot = to_categorical(y_train)\n",
    "test_Y_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Set de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_label,valid_label = train_test_split(X_train, train_Y_one_hot, test_size=0.2, random_state=13)\n",
    "classes_train = np.unique(y_train)\n",
    "nClasses_train = len(classes_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creamos el modelo de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                262176    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263,303\n",
      "Trainable params: 263,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 18:40:12.016627: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\n",
    "epochs = 50 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\n",
    "batch_size = 200 # Cantidad de imágenes que se toman a la vez en memoria\n",
    "classifier_model = Sequential()\n",
    "classifier_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(IMG_W_SIZE,IMG_H_SIZE,3)))\n",
    "classifier_model.add(LeakyReLU(alpha=0.1))\n",
    "classifier_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "classifier_model.add(Dropout(0.5))\n",
    "\n",
    "classifier_model.add(Flatten())\n",
    "classifier_model.add(Dense(32, activation='linear'))\n",
    "classifier_model.add(LeakyReLU(alpha=0.1))\n",
    "classifier_model.add(Dropout(0.5))\n",
    "classifier_model.add(Dense(nClasses_train, activation='softmax'))\n",
    "classifier_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que nuestro modelo sea más robusto utilizaremos ImageDataGenerator de tf.keras para realizar modificaciones sencillas a las imágenes de entrenamiento. En este caso, vamos a rotar las imágenes, añadir (o quitar) zoom, cambiar el brillo y la intensidad de los canales, e inclinarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=10,\n",
    "                             zoom_range=[0.95, 1.05],\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1,\n",
    "                             channel_shift_range=0.1,\n",
    "                             brightness_range=[0.95, 1.05])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhonatan/Documentos/UIDE/TRATAMIENT/Clasificacion_Imagenes/ven/lib/python3.8/site-packages/keras/optimizers/legacy/adagrad.py:84: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 18:40:21.320401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 308ms/step - loss: 1.9404 - accuracy: 0.4579 - val_loss: 1.8324 - val_accuracy: 0.5749\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 2s 273ms/step - loss: 1.9346 - accuracy: 0.5061 - val_loss: 1.8227 - val_accuracy: 0.5780\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 2s 269ms/step - loss: 1.9266 - accuracy: 0.5092 - val_loss: 1.8125 - val_accuracy: 0.5841\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 1.9195 - accuracy: 0.4793 - val_loss: 1.8010 - val_accuracy: 0.5719\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 2s 246ms/step - loss: 1.9147 - accuracy: 0.4939 - val_loss: 1.7885 - val_accuracy: 0.5688\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 1.9066 - accuracy: 0.4977 - val_loss: 1.7744 - val_accuracy: 0.5719\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 2s 232ms/step - loss: 1.9028 - accuracy: 0.4923 - val_loss: 1.7594 - val_accuracy: 0.5749\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 2s 261ms/step - loss: 1.8980 - accuracy: 0.4939 - val_loss: 1.7435 - val_accuracy: 0.5780\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 1.8868 - accuracy: 0.5107 - val_loss: 1.7254 - val_accuracy: 0.5810\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 1.8830 - accuracy: 0.4916 - val_loss: 1.7065 - val_accuracy: 0.5841\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 1.8750 - accuracy: 0.5107 - val_loss: 1.6865 - val_accuracy: 0.5841\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 1.8674 - accuracy: 0.5123 - val_loss: 1.6653 - val_accuracy: 0.5841\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 1.8586 - accuracy: 0.5145 - val_loss: 1.6424 - val_accuracy: 0.5872\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 1.8475 - accuracy: 0.5008 - val_loss: 1.6186 - val_accuracy: 0.5872\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 1.8434 - accuracy: 0.5222 - val_loss: 1.5944 - val_accuracy: 0.5872\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 1s 171ms/step - loss: 1.8288 - accuracy: 0.5299 - val_loss: 1.5699 - val_accuracy: 0.5872\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 1s 170ms/step - loss: 1.8210 - accuracy: 0.5191 - val_loss: 1.5460 - val_accuracy: 0.5872\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 1.8089 - accuracy: 0.5352 - val_loss: 1.5240 - val_accuracy: 0.5872\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 1.8021 - accuracy: 0.5191 - val_loss: 1.5032 - val_accuracy: 0.5872\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 1.7866 - accuracy: 0.5329 - val_loss: 1.4835 - val_accuracy: 0.5872\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 1.7745 - accuracy: 0.5291 - val_loss: 1.4661 - val_accuracy: 0.5872\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 1.7618 - accuracy: 0.5299 - val_loss: 1.4509 - val_accuracy: 0.5872\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 1.7466 - accuracy: 0.5391 - val_loss: 1.4384 - val_accuracy: 0.5872\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 1s 212ms/step - loss: 1.7364 - accuracy: 0.5168 - val_loss: 1.4296 - val_accuracy: 0.5872\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 1.7257 - accuracy: 0.5406 - val_loss: 1.4226 - val_accuracy: 0.5872\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 1s 170ms/step - loss: 1.7143 - accuracy: 0.5314 - val_loss: 1.4198 - val_accuracy: 0.5872\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 1.6969 - accuracy: 0.5360 - val_loss: 1.4193 - val_accuracy: 0.5872\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 1.6799 - accuracy: 0.5429 - val_loss: 1.4219 - val_accuracy: 0.5872\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 1.6766 - accuracy: 0.5237 - val_loss: 1.4273 - val_accuracy: 0.5872\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 1s 169ms/step - loss: 1.6654 - accuracy: 0.5368 - val_loss: 1.4345 - val_accuracy: 0.5872\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 1.6454 - accuracy: 0.5375 - val_loss: 1.4453 - val_accuracy: 0.5872\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 1.6252 - accuracy: 0.5444 - val_loss: 1.4585 - val_accuracy: 0.5872\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 1.6238 - accuracy: 0.5337 - val_loss: 1.4736 - val_accuracy: 0.5872\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 1.6024 - accuracy: 0.5421 - val_loss: 1.4908 - val_accuracy: 0.5872\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.5974 - accuracy: 0.5368 - val_loss: 1.5096 - val_accuracy: 0.5872\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 1.5869 - accuracy: 0.5482 - val_loss: 1.5309 - val_accuracy: 0.5872\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 1s 160ms/step - loss: 1.5765 - accuracy: 0.5391 - val_loss: 1.5523 - val_accuracy: 0.5872\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 1.5689 - accuracy: 0.5544 - val_loss: 1.5720 - val_accuracy: 0.5872\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 1.5602 - accuracy: 0.5413 - val_loss: 1.5959 - val_accuracy: 0.5872\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 1.5378 - accuracy: 0.5368 - val_loss: 1.6220 - val_accuracy: 0.5872\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 1.5561 - accuracy: 0.5413 - val_loss: 1.6432 - val_accuracy: 0.5872\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.5431 - accuracy: 0.5452 - val_loss: 1.6633 - val_accuracy: 0.5872\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 1.5299 - accuracy: 0.5452 - val_loss: 1.6883 - val_accuracy: 0.5872\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 1.5188 - accuracy: 0.5475 - val_loss: 1.7127 - val_accuracy: 0.5872\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 1.5102 - accuracy: 0.5475 - val_loss: 1.7358 - val_accuracy: 0.5872\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 1.5093 - accuracy: 0.5375 - val_loss: 1.7579 - val_accuracy: 0.5872\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 1.5072 - accuracy: 0.5559 - val_loss: 1.7811 - val_accuracy: 0.5872\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 1s 172ms/step - loss: 1.5045 - accuracy: 0.5536 - val_loss: 1.7983 - val_accuracy: 0.5872\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 1.4870 - accuracy: 0.5528 - val_loss: 1.8185 - val_accuracy: 0.5872\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 1s 161ms/step - loss: 1.4917 - accuracy: 0.5475 - val_loss: 1.8381 - val_accuracy: 0.5872\n"
     ]
    }
   ],
   "source": [
    "classifier_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "# classifier_train = classifier_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    "\n",
    "classifier_train = classifier_model.fit(datagen.flow(train_X, train_label, batch_size=batch_size),epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción para Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step\n",
      "Correct test values 45 from 810\n",
      "Porcentaje valores correctos: 5.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   1,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  48,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  97,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  45,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 458,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  19,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 114,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  28,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes_result = classifier_model.predict(X_test)\n",
    "\n",
    "predicted_classes_test=[]\n",
    "for predicted_image in predicted_classes_result:\n",
    "    predicted_classes_test.append(predicted_image.tolist().index(max(predicted_image)))\n",
    "predicted_claspredicted_classes_testses=np.array(predicted_classes_test)\n",
    "\n",
    "correct_test = np.where(predicted_classes_test==y_test)[0]\n",
    "\n",
    "print ('Correct test values %s from %s'%(len(correct_test),len(y_test)))\n",
    "print ('Porcentaje valores correctos: %.2f'%(len(correct_test)/len(y_test)*100))\n",
    "confusion_matrix(y_test, predicted_classes_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción para Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 6ms/step\n",
      "Correct train values 948 from 1633\n",
      "Porcentaje valores correctos: 58.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,  62,   0,   0,   0],\n",
       "       [  0,   0,   0, 213,   0,   0,   0],\n",
       "       [  0,   0,   0, 105,   0,   0,   0],\n",
       "       [  0,   0,   0, 948,   0,   0,   0],\n",
       "       [  0,   0,   0,  37,   0,   0,   0],\n",
       "       [  0,   0,   0, 204,   0,   0,   0],\n",
       "       [  0,   0,   0,  64,   0,   0,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes_result = classifier_model.predict(X_train)\n",
    "\n",
    "predicted_classes_train=[]\n",
    "for predicted_image in predicted_classes_result:\n",
    "    predicted_classes_train.append(predicted_image.tolist().index(max(predicted_image)))\n",
    "predicted_classes_train=np.array(predicted_classes_train)\n",
    "\n",
    "correct_train = np.where(predicted_classes_train==y_train)[0]\n",
    "\n",
    "print ('Correct train values %s from %s'%(len(correct_train),len(y_train)))\n",
    "print ('Porcentaje valores correctos: %.2f'%(len(correct_train)/len(y_train)*100))\n",
    "confusion_matrix(y_train, predicted_classes_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
