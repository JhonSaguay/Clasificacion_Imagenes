{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "IMG_H_SIZE = 32\n",
    "IMG_W_SIZE = 32\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para leer imágenes\n",
    "- En la función read_images, normalizamos cada imagen debido a que se encontró varias imágenes con un nivel de intensidad alto. Se utilizó esta ténica para obtener un mejor contraste en las imágenes con poco contraste debido al brillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(dirname):\n",
    "    imgpath = dirname + os.sep\n",
    "    images = []\n",
    "    directories = []\n",
    "    dircount = []\n",
    "    prevRoot=''\n",
    "    cant=0\n",
    "    print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(imgpath):\n",
    "        for filename in filenames:\n",
    "            if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "                cant+=1\n",
    "                filepath = os.path.join(root, filename)\n",
    "                # image = plt.imread(filepath)\n",
    "                image = cv2.imread(filepath)\n",
    "                image = cv2.resize(image, (IMG_H_SIZE, IMG_W_SIZE))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                #Función de Normalización\n",
    "                image = cv2.normalize(image, None, alpha=0,beta=200, norm_type=cv2.NORM_MINMAX)\n",
    "                images.append(image)\n",
    "                if prevRoot !=root:\n",
    "                    prevRoot=root\n",
    "                    directories.append(root)\n",
    "                    dircount.append(cant)\n",
    "                    cant=0\n",
    "    dircount.append(cant)\n",
    "\n",
    "    dircount = dircount[1:]\n",
    "    dircount[0]=dircount[0]+1\n",
    "    print('Directorios leidos:',len(directories))\n",
    "    print(\"Imagenes en cada directorio\", dircount)\n",
    "    print('Suma Total de imagenes en subdirs:',sum(dircount))\n",
    "\n",
    "    tipos=[]\n",
    "    indice=0\n",
    "    for directorio in directories:\n",
    "        name = directorio.split(os.sep)\n",
    "        print(indice , name[len(name)-1])\n",
    "        tipos.append(name[len(name)-1])\n",
    "        indice=indice+1\n",
    "\n",
    "    labels=[]\n",
    "    indice=0\n",
    "    for cantidad in dircount:\n",
    "        for i in range(cantidad):\n",
    "            labels.append(tipos[indice])\n",
    "        indice=indice+1\n",
    "\n",
    "    X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "    y = np.array(labels)\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos Sets de Entrenamiento y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo imagenes de  /home/jhonatan/Documentos/UIDE/TRATAMIENT/Clasificacion_Imagenes/CarneDataset/train/\n",
      "Directorios leidos: 7\n",
      "Imagenes en cada directorio [64, 213, 62, 204, 105, 37, 948]\n",
      "Suma Total de imagenes en subdirs: 1633\n",
      "0 CLASS_08\n",
      "1 CLASS_03\n",
      "2 CLASS_02\n",
      "3 CLASS_07\n",
      "4 CLASS_04\n",
      "5 CLASS_06\n",
      "6 CLASS_05\n",
      "leyendo imagenes de  /home/jhonatan/Documentos/UIDE/TRATAMIENT/Clasificacion_Imagenes/CarneDataset/test/\n",
      "Directorios leidos: 8\n",
      "Imagenes en cada directorio [28, 97, 48, 114, 45, 1, 19, 458]\n",
      "Suma Total de imagenes en subdirs: 810\n",
      "0 CLASS_08\n",
      "1 CLASS_03\n",
      "2 CLASS_02\n",
      "3 CLASS_07\n",
      "4 CLASS_04\n",
      "5 CLASS_01\n",
      "6 CLASS_06\n",
      "7 CLASS_05\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = read_images(os.path.join(os.getcwd(), 'CarneDataset/train'))\n",
    "X_test,y_test = read_images(os.path.join(os.getcwd(), 'CarneDataset/test'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape Imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((X_test.shape[0], IMG_H_SIZE*IMG_W_SIZE*3))\n",
    "X_train = X_train.reshape(X_train.shape[0],IMG_H_SIZE*IMG_W_SIZE*3).astype( 'float32' )\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificamos las etiquetas como enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creamos el modelo de Decision Classifier y lo entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predicted_test = model.predict(X_test)\n",
    "predicted_train = model.predict(X_train)\n",
    "correct_test = np.where(predicted_test==y_test)[0]\n",
    "correct_train = np.where(predicted_train==y_train)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = model.predict(X_test)\n",
    "predicted_train = model.predict(X_train)\n",
    "correct_test = np.where(predicted_test==y_test)[0]\n",
    "correct_train = np.where(predicted_train==y_train)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusión con Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct test values 95 from 810\n",
      "Porcentaje valores correctos: 11.73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   1,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,  48,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,  95,   1,   0,   0,   0,   0],\n",
       "       [  0,   0,  45,   0,   0,   0,   0,   0],\n",
       "       [  0,   0, 455,   1,   0,   2,   0,   0],\n",
       "       [  0,   0,  19,   0,   0,   0,   0,   0],\n",
       "       [  0,   0, 114,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,  28,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Correct test values %s from %s'%(len(correct_test),len(y_test)))\n",
    "print ('Porcentaje valores correctos: %.2f'%(len(correct_test)/len(y_test)*100))\n",
    "confusion_matrix(y_test, predicted_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusión con Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct train values 1633 from 1633\n",
      "Porcentaje valores correctos: 100.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 62,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 213,   0,   0,   0,   0,   0],\n",
       "       [  0,   0, 105,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 948,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  37,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 204,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  64]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Correct train values %s from %s'%(len(correct_train),len(y_train)))\n",
    "print ('Porcentaje valores correctos: %.2f'%(len(correct_train)/len(y_train)*100))\n",
    "confusion_matrix(y_train, predicted_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
